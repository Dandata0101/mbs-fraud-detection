{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 \n",
    "[![Author - DanRamirez](https://img.shields.io/badge/Author-DanRamirez-2ea44f?style=for-the-badge)](https://github.com/Dandata0101)\n",
    "![Python - Version](https://img.shields.io/badge/PYTHON-3.11-red?style=for-the-badge&logo=python&logoColor=white)\n",
    "[![Project Repo](https://img.shields.io/badge/Our_Project_Repo-Visit-blue?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Dandata0101/mbs-fraud-detection)\n",
    "\n",
    "I used LightGBM to explain Top impacting features to the **TARGET** variable for the training and Test Application Files. Before producing any analysis, I converted csv files in Parquet files to flatten and reduce the size of files by more than half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  flattening file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select at least 3 variables for this analysis\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from scripts.csvtopaquet import csv_to_parquet_single_file\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "csv1_file_path = os.path.join(current_directory, '01-data', 'FD_02_apl_test.csv')\n",
    "csv2_file_path = os.path.join(current_directory, '01-data', 'FD_02_apl_train.csv')\n",
    "csv3_file_path = os.path.join(current_directory, '01-data', 'FD_02_previos_appl.csv')\n",
    "\n",
    "output_file_path1 = os.path.join(current_directory, '01-data', 'FD_02_apl_test.parquet')\n",
    "output_file_path2 = os.path.join(current_directory, '01-data', 'FD_02_apl_train.parquet')\n",
    "output_file_path3 = os.path.join(current_directory, '01-data', 'FD_02_previos_appl.parquet')\n",
    "\n",
    "\n",
    "\n",
    "csv_to_parquet_single_file(csv_file_path=csv1_file_path, output_file_path=output_file_path1, chunksize=100000, sample_rows=None, drop_columns=None)\n",
    "csv_to_parquet_single_file(csv_file_path=csv2_file_path, output_file_path=output_file_path2, chunksize=100000, sample_rows=None, drop_columns=None)\n",
    "csv_to_parquet_single_file(csv_file_path=csv3_file_path, output_file_path=output_file_path3, chunksize=100000, sample_rows=None, drop_columns=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Parquet Files and assign DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parquetFile1 = os.path.join(current_directory, '01-data', 'FD_02_apl_test.parquet')\n",
    "parquetFile2 = os.path.join(current_directory, '01-data', 'FD_02_apl_train.parquet')\n",
    "parquetFile3 = os.path.join(current_directory, '01-data', 'FD_02_previos_appl.parquet')\n",
    "\n",
    "df1 = pd.read_parquet(parquetFile1)\n",
    "df1 = df1.fillna(0)\n",
    "df1.columns = df1.columns.str.replace('[^a-zA-Z0-9_]', '_')\n",
    "df1.columns = df1.columns.str.replace('/', '_')\n",
    "\n",
    "print('test Data:')\n",
    "print(df1.dtypes)\n",
    "print('')\n",
    "\n",
    "df2 = pd.read_parquet(parquetFile2)\n",
    "\n",
    "df2 = df2.fillna(0)\n",
    "df2.columns = df2.columns.str.replace('[^a-zA-Z0-9_]', '_')\n",
    "df2.columns = df2.columns.str.replace('/', '_')\n",
    "\n",
    "print('train Data:')\n",
    "print(df2.dtypes)\n",
    "print('')\n",
    "\n",
    "df3 = pd.read_parquet(parquetFile3)\n",
    "df3 = df3.fillna(0)\n",
    "df3.columns = df3.columns.str.replace('[^a-zA-Z0-9_]', '_')\n",
    "df3.columns = df3.columns.str.replace('/', '_')\n",
    "print(df3.dtypes)\n",
    "print('')\n",
    "# Load the datasets\n",
    "test  = df1\n",
    "train = df2\n",
    "\n",
    "current_data = pd.concat([train], axis=0)\n",
    "\n",
    "print('Prior Data:')\n",
    "print(df3.dtypes)\n",
    "previous_data = df3\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plotting setup\n",
    "plt.figure(figsize=(30, 12))\n",
    "\n",
    "# Horizontal Bar Chart for 'TARGET' value counts\n",
    "plt.subplot(1, 2, 1)\n",
    "target_counts = current_data['TARGET'].value_counts()\n",
    "bars = target_counts.plot(kind='barh', color=['#9e7edf', '#FFD700'])\n",
    "for index, value in enumerate(target_counts):\n",
    "    # Shadow effect for text\n",
    "    plt.text(value, index, str(value), va='center', ha='right', color='gray', fontsize=12, alpha=0.8, fontweight='bold')\n",
    "    plt.text(value-1000, index, str(value), va='center', ha='right', color='black', fontsize=12, fontweight='bold')  # Actual text\n",
    "\n",
    "# Pie Chart for 'TARGET' value counts\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['#9e7edf', '#FFD700']\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%\\n({v:d})'.format(p=pct,v=val)\n",
    "    return my_format\n",
    "\n",
    "target_counts.plot(kind='pie', colors=colors, autopct=autopct_format(target_counts), startangle=140, shadow=True, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "\n",
    "plt.ylabel('')  # Hide the 'TARGET' label on y-axis for the pie chart\n",
    "plt.title('TARGET Variable Distribution')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to not overlap subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Then proceed with training your LightGBM model\n",
    "dataset=current_data\n",
    "\n",
    "print('dataset:',dataset['TARGET'].dtypes)\n",
    "# Splitting the DataFrame into object and numeric DataFrames\n",
    "current_data_object = dataset.select_dtypes(include=['object'])\n",
    "Current_data_Numericonly = dataset.select_dtypes(include=['number'])  # This includes int, float, etc.\n",
    "current_data_object.columns.tolist()\n",
    "Current_data_Numericonly.columns.tolist()\n",
    "columns_data = dataset.columns.values.tolist()\n",
    "dummy_columns = pd.get_dummies(current_data_object, dtype=int)\n",
    "\n",
    "\n",
    "merged_data = pd.concat([Current_data_Numericonly, dummy_columns], axis=1)\n",
    "# Drop 'SK_ID_CURR' and columns with blank names\n",
    "merged_data.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "def clean_column_names(df):\n",
    "    df.columns = [col.replace('{', '')\n",
    "                     .replace('}', '')\n",
    "                     .replace('[', '')\n",
    "                     .replace(']', '')\n",
    "                     .replace('\"', '')\n",
    "                     .replace(':', '')\n",
    "                     .replace(',', '') for col in df.columns]\n",
    "    return df\n",
    "\n",
    "# Clean the column names of your DataFrame\n",
    "merged_data = clean_column_names(merged_data)\n",
    "\n",
    "print(merged_data.dtypes)\n",
    "\n",
    "sample=merged_data.head(1)\n",
    "print(merged_data.head(1).T)\n",
    "\n",
    "sample.to_csv('testmerge.csv')\n",
    "\n",
    "print(Current_data_Numericonly.dtypes)\n",
    "\n",
    "# To see the columns of each DataFrame\n",
    "print(\"Object Columns:\", len(current_data_object.columns.tolist()))\n",
    "print(\"Numeric Columns:\", len(Current_data_Numericonly.columns.tolist()))\n",
    "\n",
    "columns_data[(columns_data == 'current_data')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "merged_data\n",
    "merged_data = merged_data.fillna(0)\n",
    "# Assuming your DataFrame and target variable setup\n",
    "X = merged_data.drop('TARGET', axis=1)\n",
    "y = merged_data['TARGET']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "}\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = lgb.early_stopping(stopping_rounds=10)\n",
    "\n",
    "# Training the model\n",
    "num_round = 100\n",
    "bst = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=num_round,\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "y_pred_proba = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred_proba]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Valuesâ€” Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# You should replace 'model' with 'bst', which is your trained LightGBM booster\n",
    "explainer = shap.TreeExplainer(bst)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Now you can generate the SHAP summary plot with the correct SHAP values\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\", show=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Impact of Features on Model Predictions with SHAP Values\")\n",
    "plt.gcf().set_size_inches(10, 8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Summary Plot Analysis for Fraud Detection Model\n",
    "\n",
    "# SHAP Value Chart Summary\n",
    "\n",
    "## Overview\n",
    "\n",
    "The SHAP summary plot illustrates the impact of various features on a predictive model's output, indicating how each feature contributes to the model's predictions.\n",
    "\n",
    "## Key Features by Impact\n",
    "\n",
    "- **EXT_SOURCE_3**: Shows the most considerable impact on model predictions, generally contributing to higher prediction values when the feature value is high.\n",
    "- **EXT_SOURCE_2 and EXT_SOURCE_1**: Also significant, these features exhibit a mix of high and low SHAP values.\n",
    "\n",
    "## Observations on Feature Impact\n",
    "\n",
    "- Features like `AMT_GOODS_PRICE`, `DAYS_EMPLOYED`, and `DAYS_BIRTH` demonstrate varied impacts on model output, suggesting complex interactions.\n",
    "- The color intensity of the points (pink for higher values and blue for lower values) reflects the feature's value, indicating its influence on the prediction.\n",
    "\n",
    "## Summary of Insights\n",
    "\n",
    "External source features notably influence prediction outcomes, with the model heavily weighing these attributes. The variability in SHAP values for features like `AMT_GOODS_PRICE`, `DAYS_EMPLOYED`, and `DAYS_BIRTH` suggests that their impact on the model's predictions changes across different observations.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The SHAP summary plot highlights external source features as key drivers in the model's predictive capabilities, with personal client attributes also providing substantial influence. These insights are valuable for understanding the model's behavior, guiding feature engineering efforts, and potentially enhancing model performance.\n",
    "\n",
    "\n",
    "### Plot Interpretation\n",
    "- The color of the dots (from blue to red) corresponds to the feature value from low to high.\n",
    "- The positioning of the dots on the x-axis indicates the SHAP value's impact on the model's output.\n",
    "- Red dots positioned to the right suggest that higher feature values contribute to an increased likelihood of fraud.\n",
    "- Blue dots to the left suggest that lower feature values contribute to an increased likelihood of fraud.\n",
    "- The center area, where blue and red dots overlap, shows where feature values do not clearly distinguish between fraud and non-fraud cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. Column charting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Column Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = current_data\n",
    "#file = file[file[\"TARGET\"]==1.0]\n",
    "    \n",
    "def distributionGraphs(x):\n",
    "    print(\"\\033[1m\\033[1;3mDistribution Based on \"+str(x)+\"\\033[0m\")\n",
    "    print('Description : ',columns_data[(columns_data == 'current_data') & (columns_data == x)],'\\n')\n",
    "    \n",
    "    #Create a normalized value count converted to percentage\n",
    "    target_group = round(file.groupby('TARGET')[x].value_counts(normalize=True,sort=False)*100)\n",
    "    \n",
    "    #cnt = int((target_group.count()))\n",
    "    cnt = int((target_group.count()/2))\n",
    "        \n",
    "    all_colr = ['#F38181','#FCE38A','#EAFFD0','#95E1D3','#EEEEEE','#00ADB5']\n",
    "    colors = all_colr[:cnt]\n",
    "    plt.figure(figsize=(30,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title(str(x)+' Distribution grouped by Target')\n",
    "    ax = target_group.plot(kind='bar',color=colors)\n",
    "    \n",
    "    # A method to print values in Bar\n",
    "    for bar in ax.patches:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height()/2, str(bar.get_height()))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title(str(x)+' distribution in Overall Records')\n",
    "    (current_data[x].value_counts(normalize=True)*100).plot(kind='pie', autopct = \"%1.0f%%\", colors=colors)\n",
    "    plt.ylabel(' ')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\033[1m\\033[1;3mTable View\\033[0m\")\n",
    "    print(target_group)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "for feature in current_data_object[1:5]: #add column list numbers to control [1:2]\n",
    "    distributionGraphs(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Column Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# NUMERICAL ANALYSIS\n",
    "# =============================================================================\n",
    "##REMOVING THE COLUMNS WITH NAME STARTING WITH\"FLAG\"\n",
    "#Creating list of Flag columns and removing from numeric columns\n",
    "data_numeric_cols = list(Current_data_Numericonly)\n",
    "data_flag_cols = [val for val in data_numeric_cols if 'FLAG_' in val]\n",
    "\n",
    "# remove flags from numeric cols\n",
    "for val in data_flag_cols:\n",
    "    data_numeric_cols.remove(val)\n",
    "\n",
    "data_numeric_cols\n",
    "\n",
    "#REMOVING ALSO REGION AND CITY FROM NUMERIC\n",
    "data_area_cols = [val for val in data_numeric_cols if ('REGION_' in val or 'CITY_' in val )]\n",
    "\n",
    "for val in data_area_cols:\n",
    "    data_numeric_cols.remove(val)\n",
    "data_numeric_cols\n",
    "\n",
    "#REMOVING HOUR AND MINUTE\n",
    "data_days_cols = [val for val in data_numeric_cols if ('DAYS_' in val or 'HOUR_' in val )]\n",
    "\n",
    "for val in data_days_cols:\n",
    "    data_numeric_cols.remove(val)\n",
    "data_numeric_cols\n",
    "\n",
    "\n",
    "\n",
    "#REMOVING ALL COLUMNS RELATED TO AMOUNT\n",
    "data_amt_cols = [val for val in data_numeric_cols if 'AMT_' in val]\n",
    "\n",
    "for val in data_amt_cols:\n",
    "    data_numeric_cols.remove(val)\n",
    "\n",
    "#LISTING THE VARIABLES DELETED FROM OUR NUMERICAL COLUMN\n",
    "print('Numeric cols',len(data_numeric_cols))\n",
    "print('Area cols',len(data_area_cols))\n",
    "print('Flag cols',len(data_flag_cols))\n",
    "print('Days cols',len(data_days_cols))\n",
    "print('Amount cols',len(data_amt_cols))\n",
    "\n",
    "#REMOVING THE TARGET AND CONSUMER ID \n",
    "#list(data_numeric_cols).remove('SK_ID_CURR')\n",
    "data_numeric_cols.remove('TARGET')\n",
    "\n",
    "# Let's see how values are distributed in numeric cols\n",
    "current_data_info = round(current_data[data_numeric_cols+data_amt_cols+data_days_cols].describe().T,2)\n",
    "current_data_info['description'] = current_data_info.index.map(lambda x: columns_data[(columns_data == 'application_data') & (columns_data == x)])\n",
    "current_data_info\n",
    "\n",
    "# We have verified the distribution during the data cleaning stage. We'll create a function now to visualize the distribution of numeric values\n",
    "\n",
    "def numericDistributionGraph(col):\n",
    "    print(\"\\033[1m\\033[1;3mDistribution Based on \"+str(col)+\"\\033[0m\")\n",
    "    print('Description : ',columns_data[(columns_data == 'application_data') & (columns_data == col)])\n",
    "    \n",
    "    plt.figure(figsize=(18,10))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    sns.histplot(current_data[col], bins=10, color='#222831')\n",
    "    plt.title('Distribution of '+col+' Across current application', fontsize=8)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    sns.boxplot(x='TARGET', y=col ,data=current_data,)\n",
    "    plt.title('Distribution of '+col+' with respect to TARGET feature', fontsize=8)\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    sns.boxplot(x=current_data[col],color='#F38181')\n",
    "    plt.title('Distribution of '+col+' quantile Across current application', fontsize=8)\n",
    "    \n",
    "\n",
    "\n",
    "for col in data_numeric_cols[2:3]:\n",
    "    numericDistributionGraph(col)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#EXERCISE:\n",
    "#    Use new features and make your analysis. \n",
    "# \n",
    "# =============================================================================\n",
    "\n",
    "##Example of specific variables: Income external sources\n",
    "def distGraphs(x):\n",
    "    print(\"\\033[1m\\033[1;3mDistribution Based on \"+str(x)+\"\\033[0m\")\n",
    "    print('Description : ',columns_data[(columns_data == 'current_data') & (columns_data == x)],'\\n')\n",
    "    \n",
    "    #Create a normalized value count converted to percentage\n",
    "    target_group = round(file.groupby('TARGET')[x].value_counts(normalize=True,sort=False)*100)\n",
    "    \n",
    "    plt.figure(figsize=(30,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title(str(x)+' Distribution grouped by Target')\n",
    "    ax = target_group.plot(kind='bar')\n",
    "    \n",
    "    for bar in ax.patches:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height()/2, str(bar.get_height()))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title(str(x)+' distribution in Overall Records')\n",
    "    (current_data[x].value_counts(normalize=True)*100).plot(kind='pie', autopct = \"%1.0f%%\")\n",
    "    plt.ylabel(' ')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\033[1m\\033[1;3mTable View\\033[0m\")\n",
    "    print(target_group)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(x='TARGET', y='EXT_SOURCE_1' ,data=current_data,)\n",
    "plt.title('Distribution of EXT_SOURCE_1 with respect to TARGET feature')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(x='TARGET', y='EXT_SOURCE_2' ,data=current_data,)\n",
    "plt.title('Distribution of EXT_SOURCE_2 with respect to TARGET feature')\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(x='TARGET', y='EXT_SOURCE_3' ,data=current_data,)\n",
    "plt.title('Distribution of EXT_SOURCE_3 with respect to TARGET feature')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for col in data_flag_cols:\n",
    "    distGraphs(col)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraudenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
